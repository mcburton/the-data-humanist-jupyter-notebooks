{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('thought', 26), ('poor', 11), ('cried', 7), ('little', 3), ('exclaimed', 3), ('inquired', 1), ('interrupted', 1), ('noticed', 1), ('shouted', 1), ('pleaded', 1), ('replied', 1), ('upon', 1), ('red', 1), ('different', 1), ('foolish', 1), ('anxious', 1), ('miss', 1)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "with open('alice.txt') as a:\n",
    "    alice_text = a.read()\n",
    "alice_tokens = nltk.word_tokenize(alice_text.lower())\n",
    "alice_pos = nltk.pos_tag(alice_tokens)\n",
    "alice_pos = [i for i in alice_pos if i[0].isalpha()]\n",
    "adjs = []\n",
    "for i, j in enumerate(alice_pos):\n",
    "    if j[0]=='alice':\n",
    "        before = i-1\n",
    "        if alice_pos[before][1] == 'JJ':\n",
    "            adjs.append(alice_pos[before][0])\n",
    "from collections import Counter\n",
    "print(Counter(adjs).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('alice', 'ventured')\n",
      "('alice', 'indignantly')\n",
      "('exclaimed', 'alice')\n",
      "('poor', 'alice')\n",
      "('thought', 'alice')\n",
      "('cried', 'alice')\n",
      "('together', 'alice')\n",
      "('alice', 'replied')\n",
      "('alice', 'waited')\n",
      "('said', 'alice')\n",
      "('alice', 'hastily')\n",
      "('alice', 'felt')\n",
      "('alice', 'looked')\n",
      "('alice', 'asked')\n",
      "('alice', 'thought')\n",
      "('alice', 'could')\n",
      "('alice', 'began')\n",
      "('alice', 'rather')\n",
      "('caterpillar', 'alice')\n",
      "('alice', 'heard')\n",
      "('alice', 'quite')\n",
      "('alice', 'must')\n",
      "('alice', 'went')\n",
      "('alice', 'said')\n",
      "('little', 'alice')\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "at = [i for i in alice_tokens if i.isalpha()]\n",
    "finder = BigramCollocationFinder.from_words(at)\n",
    "finder.apply_freq_filter(3)\n",
    "ignored_words = nltk.corpus.stopwords.words('english')\n",
    "finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
    "terms = []\n",
    "for i in finder.nbest(bigram_measures.pmi, 10000):\n",
    "    if \"alice\" in i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('last', 3), ('queen', 3), ('hot', 1), ('quick', 1), ('kitten', 1), ('safe', 1), ('poor', 1), ('o', 1)]\n",
      "[('queen', 4), ('last', 1), ('ready', 1), ('adventurous', 1), ('welcome', 1), ('open', 1), ('i', 1)]\n",
      "[('come', 1), ('interested', 1), ('usual', 1), ('pleased', 1), ('south', 1), ('quick', 1)]\n",
      "[('cried', 3), ('exclaimed', 2), ('poor', 2), ('last', 1), ('flattered', 1), ('fair', 1), ('henceforth', 1), ('tinkle', 1), ('cold', 1), ('open', 1), ('awful', 1), ('i', 1), ('cocky', 1), ('fancy', 1), ('nice', 1), ('o', 1), ('john', 1), ('lantern', 1), ('like', 1)]\n",
      "[('thought', 26), ('poor', 11), ('cried', 7), ('little', 3), ('exclaimed', 3), ('inquired', 1), ('interrupted', 1), ('noticed', 1), ('shouted', 1), ('pleaded', 1), ('replied', 1), ('upon', 1), ('red', 1), ('different', 1), ('foolish', 1), ('anxious', 1), ('miss', 1)]\n"
     ]
    }
   ],
   "source": [
    "def adjectives_to_the_left(text, character):\n",
    "    import nltk\n",
    "    with open(text) as a:\n",
    "        my_text = a.read()\n",
    "    my_tokens = nltk.word_tokenize(my_text.lower())\n",
    "    my_pos = nltk.pos_tag(my_tokens)\n",
    "    my_pos = [i for i in my_pos if i[0].isalpha()]\n",
    "    adjs = []\n",
    "    for i, j in enumerate(my_pos):\n",
    "        if j[0]==character:\n",
    "            before = i-1\n",
    "            if my_pos[before][1] == 'JJ':\n",
    "                adjs.append(my_pos[before][0])\n",
    "    from collections import Counter\n",
    "    return(Counter(adjs).most_common())\n",
    "\n",
    "comparison_texts = [('lww.txt', 'lucy'), ('lww.txt', 'susan'), ('oz.txt', 'dorothy'), ('peter_and_wendy.txt', 'wendy'), ('alice.txt', 'alice')]\n",
    "for i, j in comparison_texts:\n",
    "    result = adjectives_to_the_left(i, j)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.collocations\n",
    "def collocation_analysis(text, character):    \n",
    "    with open(text) as a:\n",
    "        my_text = a.read()\n",
    "    my_tokens = nltk.word_tokenize(my_text.lower())\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    at = [i for i in my_tokens if i.isalpha()]\n",
    "    finder = BigramCollocationFinder.from_words(at)\n",
    "    finder.apply_freq_filter(3)\n",
    "    ignored_words = nltk.corpus.stopwords.words('english')\n",
    "    finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
    "    terms = []\n",
    "    pairs = []\n",
    "    for i in finder.nbest(bigram_measures.pmi, 10000):\n",
    "        if character in i:\n",
    "            pairs.append(i)\n",
    "    return pairs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "output = []\n",
    "for i, j in comparison_texts:\n",
    "    pairs = collocation_analysis(i, j)\n",
    "    output.append(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terms = []\n",
    "for j,l in enumerate(output):\n",
    "    t = []\n",
    "    for o in l:\n",
    "        if o[0] == comparison_texts[j][1]:\n",
    "            t.append(o[1])\n",
    "        else:\n",
    "            t.append(o[0])\n",
    "    terms.append(t)\n",
    "    \n",
    "term_set = list(set([val for sublist in terms for val in sublist]))\n",
    "binary_output = []\n",
    "\n",
    "columns = [\"term\"]\n",
    "columns.extend([j[1] for j in comparison_texts])\n",
    "for term in term_set:\n",
    "    binaries = []\n",
    "    binaries.append(term)\n",
    "    for group_of_terms in terms:\n",
    "        \n",
    "        if term in group_of_terms:\n",
    "            binaries.append(\"XXXXX\")\n",
    "        else: \n",
    "            binaries.append(\"-----\")\n",
    "    binary_output.append(binaries)\n",
    "df = pd.DataFrame(binary_output, columns=columns)\n",
    "df.to_csv('comparison.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#each row is a term\n",
    "#each column is a text\n",
    "#cells are just x or empty\n",
    "#allows you to see common, unique, and split\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
